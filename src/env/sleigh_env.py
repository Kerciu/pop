import gymnasium as gym

from core.actions import Action, solve_knapsack_greedy
from core.distance_utils import distance
from core.simulator import Simulator
from env.state_encoder import StateEncoder
from models.coordinate import Coordinate
from models.problem import Problem
from models.sleigh_state import SleighState
from models.velocity import Velocity


class SleighEnv(gym.Env):
    def __init__(self, problem: Problem, simulator: Simulator):
        self.problem = problem
        self.sim = simulator

        # --- AUTOMATYCZNE SKALOWANIE MAPY ---
        max_coord = 100.0  # Domylne minimum
        for g in problem.gifts:
            max_coord = max(max_coord, abs(g.destination.c), abs(g.destination.r))

        # Dodajemy 20% marginesu, 偶eby agent nie uderza w cian przy krawdzi
        self.map_limit = max_coord * 1.2
        print(f" Wykryto rozmiar mapy: +/- {self.map_limit:.1f}")
        # ------------------------------------

        # Przekazujemy wykryty limit do Encodera
        self.encoder = StateEncoder(problem, simulator, map_limit=self.map_limit)

        self.state = None
        self.prev_dist = 0.0
        self.gifts_map = {g.name: g for g in problem.gifts}
        self.MAX_FUEL = 300

        self.ACTION_MAPPING = {
            0: Action.AccUp,
            1: Action.AccDown,
            2: Action.AccLeft,
            3: Action.AccRight,
            4: Action.Floating,
            5: Action.LoadCarrots,
            6: Action.LoadGifts,
            7: Action.DeliverGift,
        }
        self.action_space_size = len(self.ACTION_MAPPING)

    def reset(self):
        initial_gifts = [g.name for g in self.problem.gifts]
        self.state = SleighState(
            current_time=0,
            position=Coordinate(0, 0),
            velocity=Velocity(0, 0),
            sleigh_weight=10,
            carrot_count=100,
            loaded_gifts=[],
            available_gifts=initial_gifts,
            delivered_gifts=[],
            last_action_was_acceleration=False,
        )
        target_pos = self._get_target_pos()
        self.prev_dist = distance(self.state.position, target_pos)
        return self.encoder.encode(self.state)

    def _get_target_pos(self):
        if self.state.loaded_gifts:
            g_name = self.state.loaded_gifts[0]
            if g_name in self.gifts_map:
                return self.gifts_map[g_name].destination
        return self.sim.lapland_pos

    def _sort_loaded_gifts(self):
        """Sortuje zaadowane prezenty od najbli偶szego."""
        if not self.state.loaded_gifts:
            return
        current_pos = self.state.position
        self.state.loaded_gifts.sort(
            key=lambda g_name: distance(current_pos, self.gifts_map[g_name].destination)
        )

    def step(self, action_id: int):
        # Domylna kara za upyw czasu (motywacja do popiechu)
        reward = -0.1
        step_penalty = 0.0

        action_enum = self.ACTION_MAPPING[action_id]
        param = 1

        # --- 1. RUCH (Akceleracja) ---
        if action_enum in [
            Action.AccUp,
            Action.AccDown,
            Action.AccLeft,
            Action.AccRight,
        ]:
            max_acc = self.sim.accel_table.get_max_acceleration_for_weight(
                self.state.sleigh_weight
            )

            # KARA: Pr贸ba ruchu bez paliwa lub przy przeci偶eniu
            if self.state.carrot_count <= 0 or max_acc == 0:
                reward -= 10.0  # Du偶a kara za pr贸b ruchu "na pusto"
                # Fizycznie nic si nie dzieje (lub dryfujemy), ale agent musi si nauczy
                # W symulatorze wywoamy Floating, 偶eby gra si nie sypaa, ale kara jest
                try:
                    self.state = self.sim.apply_action(self.state, Action.Floating, 1)
                except:
                    pass
            else:
                param = max_acc
                # Wykonanie normalnego ruchu
                try:
                    self.state = self.sim.apply_action(self.state, action_enum, param)
                except Exception:
                    # Np. zamanie zasady Acc -> Float -> Acc
                    reward -= 20.0
                    # Musimy "odczeka" kar dryfujc
                    try:
                        self.state = self.sim.apply_action(
                            self.state, Action.Floating, 1
                        )
                    except:
                        pass

        # --- 2. TANKOWANIE ---
        elif action_enum == Action.LoadCarrots:
            dist_base = distance(self.state.position, self.sim.lapland_pos)

            # KARA: Tankowanie poza baz
            if dist_base > self.problem.D:
                reward -= 50.0
                # Czas pynie (zmarnowana tura na pr贸bie tankowania)
                self.state.current_time += 1
            else:
                needed = max(0, 100 - self.state.carrot_count)
                # KARA/NAGRODA: Tankowanie
                if needed > 0:
                    param = needed
                    # Nagroda jest maa, 偶eby nie farmi punkt贸w tankowaniem
                    reward += 1.0
                    self.state = self.sim.apply_action(self.state, action_enum, param)
                else:
                    # Peny bak - kara za marnowanie czasu
                    reward -= 10.0
                    self.state.current_time += 1  # Czas pynie

        # --- 3. DOSTARCZANIE ---
        elif action_enum == Action.DeliverGift:
            if not self.state.loaded_gifts:
                # KARA: Pr贸ba dostarczenia pustego worka
                reward -= 20.0
                self.state.current_time += 1
            else:
                target = self.gifts_map[self.state.loaded_gifts[0]]
                dist_to_target = distance(self.state.position, target.destination)

                if dist_to_target > self.problem.D:
                    # KARA: Pr贸ba zrzutu za daleko od celu
                    reward -= 20.0
                    self.state.current_time += 1
                else:
                    # SUKCES: Dostarczamy
                    param = 0
                    prev_delivered = len(self.state.delivered_gifts)
                    self.state = self.sim.apply_action(self.state, action_enum, param)

                    # Sortujemy reszt, 偶eby nowy cel by [0]
                    self._sort_loaded_gifts()

                    # Sprawdzamy czy si udao (dla pewnoci)
                    if len(self.state.delivered_gifts) > prev_delivered:
                        reward += 5000.0  # WIELKA NAGRODA

        # --- 4. ADOWANIE PREZENTW ---
        elif action_enum == Action.LoadGifts:
            dist_base = distance(self.state.position, self.sim.lapland_pos)

            # KARA: adowanie poza baz
            if dist_base > self.problem.D:
                reward -= 50.0
                self.state.current_time += 1

            # KARA: adowanie, gdy ju偶 mamy prezenty (w tym uproszczonym modelu lecimy full -> empty -> full)
            # To zapobiega ptli adowania w nieskoczono
            elif self.state.loaded_gifts:
                reward -= 50.0  # Bardzo bolesna kara za ptl!
                self.state.current_time += 1

            # KARA: Brak prezent贸w do wzicia
            elif not self.state.available_gifts:
                reward -= 10.0
                self.state.current_time += 1

            else:
                # SUKCES: Pr贸ba zaadunku
                real_max_weight = self.sim.accel_table.ranges[-1].max_weight_inclusive
                gifts_to_load_ids = solve_knapsack_greedy(
                    self.state.available_gifts,
                    self.gifts_map,
                    real_max_weight,
                    self.state.sleigh_weight,
                )

                if not gifts_to_load_ids:
                    # Nic si nie zmiecio (mao prawdopodobne przy pustych saniach)
                    reward -= 10.0
                    self.state.current_time += 1
                else:
                    # adujemy manualnie (bez upywu czasu)
                    try:
                        loaded_count = 0
                        loaded_set = set(gifts_to_load_ids)
                        new_available = []
                        for g_name in self.state.available_gifts:
                            if g_name in loaded_set:
                                gift = self.gifts_map[g_name]
                                self.state.loaded_gifts.append(g_name)
                                self.state.sleigh_weight += gift.weight
                                loaded_count += 1
                            else:
                                new_available.append(g_name)
                        self.state.available_gifts = new_available

                        self._sort_loaded_gifts()
                        reward += 10.0 + loaded_count * 0.5  # Nagroda za zaadunek
                    except:
                        reward -= 50.0  # Krytyczny bd

        # --- 5. FLOATING (Dryfowanie) ---
        elif action_enum == Action.Floating:
            self.state = self.sim.apply_action(self.state, action_enum, 1)
            # Maa kara za strat czasu, chyba 偶e czekamy na co sensownego
            reward -= 0.1

        # --- FINALIZE STEP (Wsp贸lne obliczenia) ---
        return self._finalize_step_logic(reward)

    def _finalize_step_logic(self, reward):
        target_pos = self._get_target_pos()
        curr_dist = distance(self.state.position, target_pos)

        # Prdko
        velocity_mag = (self.state.velocity.vc**2 + self.state.velocity.vr**2) ** 0.5

        # Nagroda za zbli偶anie si (tylko przy rozsdnej prdkoci)
        # Aby zachci do latania w dobr stron, ale nie premiowa pdu
        dist_improvement = self.prev_dist - curr_dist
        if abs(dist_improvement) < 1000 and velocity_mag < 80:
            reward += dist_improvement * 0.2

        self.prev_dist = curr_dist

        # Warunki koca gry
        done = False

        # 1. Koniec czasu
        if self.state.current_time >= self.problem.T:
            done = True

        # 2. Wszystko dostarczone
        if len(self.state.delivered_gifts) == len(self.problem.gifts):
            done = True
            reward += 100000.0  # Jackpot

        # 3. Wylot poza map
        if (
            abs(self.state.position.c) > self.map_limit
            or abs(self.state.position.r) > self.map_limit
        ):
            done = True
            reward -= 200.0  # mier

        return self.encoder.encode(self.state), reward, done, {}
